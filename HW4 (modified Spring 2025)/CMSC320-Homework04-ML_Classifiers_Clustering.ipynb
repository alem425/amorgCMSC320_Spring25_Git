{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIHQ7pmwhmjZ"
   },
   "source": [
    "# **Exploring Top Restaurants in Las Vegas: A Data Science Journey!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yKYTZZ6hqy7"
   },
   "source": [
    "## **Background:**\n",
    "As we gear up for our summer trip to Las Vegas, we've decided to delve into the city's vibrant dining scene. To ensure we experience the best and most budget-friendly eateries, we've utilized the [Yelp dataset](https://business.yelp.com/data/resources/open-dataset/) to identify top-rated restaurants in Las Vegas. We've also filtered out the top 50 fast-food chains to avoid the usual options and embrace more unique culinary experiences.​\n",
    "\n",
    "### **Project Goal/Stakes:**\n",
    "Our objective is to analyze the Yelp dataset to pinpoint exceptional yet affordable dining establishments in Las Vegas. By focusing on local favorites and hidden gems, we aim to curate a list of restaurants that offer both quality and value, ensuring our trip is both delicious and economical.​\n",
    "\n",
    "### **Project Submission:**\n",
    "A Note from your Backpacking Partner:\n",
    "\n",
    "\"Hello, team. I've gathered and preprocessed the Yelp dataset for Las Vegas, filtering out the top 50 fast-food chains to ensure we explore the city's unique culinary offerings. Now, it's up to you to analyze this data and find the best budget-friendly restaurants for our trip. I request that you do NOT delete/modify any part of the question description, given code, and pre-loaded comments in the code block.\n",
    "\n",
    "We look forward to your insightful analyses and recommendations for our culinary adventure in Las Vegas! Hope we can have a wonderful trip in Vegas.\"\n",
    "\n",
    "\n",
    "Just a reminder, for a better a smooth autograder experience, please:\n",
    "- Write your answers between <font color='green'>`# ANSWER STARTING HERE`</font> and <font color='green'>`# ANSWER ENDS HERE`</font>.\n",
    "- Do not delete helper cells\n",
    "- Always use `seed=42` as your `random_state` in the whole notebook.\n",
    "\n",
    "Submit this Google Colab/Jupyter Notebook file on Gradescope. Cheers!🥂\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kwrcpIghvV0"
   },
   "source": [
    "### **Part A (4 Pts Total)**\n",
    "\n",
    "#### You are expected to load the Yelp data and provide some metrics so I know what we are working with. Follow these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrZpXNu0h0KE"
   },
   "source": [
    "##### ***1.1 Import Libraries (0 pt)***\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Many libraries are imported for you. Import the additional **Sci-kit learn**  modules:\n",
    "- **Decision Tree Classifier**\n",
    "- **K Neighbors Classifier**\n",
    "- **Logistic Regression**\n",
    "- **Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDkWLUDzhOBI"
   },
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import ast\n",
    "pd.set_option('display.max_columns', None)\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings # To suppress some warnings\n",
    "\n",
    "# Suppress the specific FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n",
    "\n",
    "# Add the classifiers to the imports here:\n",
    "# ANSWER STARTING HERE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# ANSWER ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmFxJQ2Kh7YR"
   },
   "source": [
    "##### ***1.2 Understanding the Dataset (1 pt)***\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "1. Load the dataset to a dataframe named `yelp_data`.\n",
    "2. Print the dimensions of the dataset.\n",
    "3. Print the column names of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wN3R1X8yhrdW"
   },
   "outputs": [],
   "source": [
    "# Load the yelp dataset\n",
    "# ANSWER STARTING HERE\n",
    "yelp_data =\n",
    "\n",
    "# ANSWER ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TESJqift-khn"
   },
   "outputs": [],
   "source": [
    "yelp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrM1h-tSh_vh"
   },
   "source": [
    "##### ***1.3 Explore the Data (3 pts)***\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Let's see what is stored in the dataset first to get a feel for what we can expect. Write only a **single line** of code for each.\n",
    "1. Print the number of **samples** in the dataset (a numerical value).\n",
    "2. Print the number of **features** in the dataset (a numerical value)\n",
    "3. **Create two lists of the unique business names and categories**, named `names` and `categories`, respectively.\n",
    "\n",
    "Your answer should be similar to:\n",
    "- Business: [\"GameWorks\", \"Divine Cafe\", \"Trattoria Italia\", \"Water\", ...]\n",
    "- Rarities: [\"Standard\", \"Mythic\", \"Legendary\", \"Ultra-Beast\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGlha5bNmTIz"
   },
   "outputs": [],
   "source": [
    "# Print the number of samples\n",
    "# ANSWER STARTING HERE\n",
    "\n",
    "\n",
    "# ANSWER ENDS HERE\n",
    "\n",
    "# Print the number of features\n",
    "# ANSWER STARTING HERE\n",
    "\n",
    "\n",
    "# ANSWER ENDS HERE\n",
    "\n",
    "# Write code to get your two lists.\n",
    "# ANSWER STARTING HERE\n",
    "names =\n",
    "\n",
    "categories=\n",
    "\n",
    "# ANSWER ENDS HERE\n",
    "\n",
    "print(\"Business:\", names)\n",
    "print(len(names))\n",
    "print(\"Categories:\", categories)\n",
    "print(len(categories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWIXqii1puOD"
   },
   "source": [
    "### **Part B (9 Pts Total)**\n",
    "\n",
    "#### Now we need to do some scraping of the dataset so you can feed it into the training model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68_nWfiTADX4"
   },
   "source": [
    "##### ***2.1 Preprocessing (3 pts)***\n",
    "\n",
    "---\n",
    "Before training the model, we need to clean and prepare the data. Please do the following.\n",
    "1. Print the number of missing data in each column. Please specify which column name you got the number from. You can create a dictionary `missing_data`.\n",
    "2. Handle the missing values in the `yelp_data` dataset the best way you can think of. Save the processed file as `yelp_data`.\n",
    "\n",
    "*Suggestions:* Fill in empty with 0s, or do some type of imputation technique such as fill in the empty data with the most common or average of data in that column, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ej6CKz3BAfnd"
   },
   "outputs": [],
   "source": [
    "# Print the number of missing data in each column.\n",
    "# ANSWER STARTING HERE\n",
    "\n",
    "missing_data =\n",
    "\n",
    "# ANSWER ENDS HERE\n",
    "\n",
    "\n",
    "# Write a code that handles missing values in the dataset by discarding them\n",
    "# ANSWER STARTING HERE\n",
    "\n",
    "yelp_data =\n",
    "\n",
    "# ANSWER ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZzbXaceD2rV"
   },
   "source": [
    "##### ***2.2 Splitting, Training, and Testing (6 pts)***\n",
    "\n",
    "---\n",
    "\n",
    "Typically in machine learning, we wish to split the data into various portions in order to let a machine learning model train itself on one part of the data and test itself on another part of the data. Splitting also helps prevent overfitting, ensuring the model can make accurate predictions on new, unseen data.\n",
    "\n",
    "Training data is used to learn the model, while testing data assesses its generalization to unseen examples. I would recommend using **80%** of the data  for training and **20%** for testing.\n",
    "\n",
    "Before we split the data, we are going to help you process the raw data a little bit more:\n",
    " 1. We prefer restaurants close to Canyon Gate in Las Vegas, near our AirBnb.\n",
    " 2. We add a `preference_rating` column at the end of the dataframe, so that we can predict the rating classification.\n",
    " 3. We add non-linearity to `preference_rating`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PML17T7CCcHC"
   },
   "outputs": [],
   "source": [
    "# Helper Function, Do Not Remove\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def business_id_to_float(business_id):\n",
    "    hash_object = hashlib.md5(business_id.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    hash_int = int(hash_hex[:8], 16)\n",
    "    return hash_int / (2**32 - 1)\n",
    "\n",
    "def float_to_business_id(unique_float, original_business_ids):\n",
    "    id_to_float = {id: business_id_to_float(id) for id in original_business_ids}\n",
    "    closest_id = min(id_to_float.keys(), key=lambda x: abs(id_to_float[x] - unique_float))\n",
    "    return closest_id\n",
    "\n",
    "# Non-linear transformation functions\n",
    "def apply_non_linear_transformation(x, steepness=5):\n",
    "    \"\"\"\n",
    "    Apply a sigmoid-like non-linear transformation\n",
    "    - Increases sensitivity around mid-range values\n",
    "    - Compresses extreme values\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-steepness * (x - 0.5))) * 10\n",
    "\n",
    "def exponential_boost(x, base=2):\n",
    "    \"\"\"\n",
    "    Apply exponential boosting to favor higher values\n",
    "    \"\"\"\n",
    "    return np.power(x, base)\n",
    "\n",
    "def angular_score_adjustment(x):\n",
    "    \"\"\"\n",
    "    Use trigonometric function to add non-linear variation\n",
    "    \"\"\"\n",
    "    return np.sin(np.pi * x / 2) * 10\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'location_score': 0.25,\n",
    "    'business_stars': 0.25,\n",
    "    'reviewer_stars': 0.25,\n",
    "    'engagement_score': 0.25\n",
    "}\n",
    "\n",
    "# Define the target location as Canyon Gate, Las Vegas\n",
    "target_location = (36.1433, -115.2821)\n",
    "\n",
    "# Haversine distance calculation\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2)**2\n",
    "    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "# Main processing function\n",
    "def generate_preference_rating(yelp_data):\n",
    "    # Calculate distance from target location\n",
    "    yelp_data['distance_km'] = yelp_data.apply(\n",
    "        lambda row: haversine(target_location[0], target_location[1],\n",
    "                               row['latitude'], row['longitude']), axis=1\n",
    "    )\n",
    "\n",
    "    # Normalize distance to [0, 1] and invert (closer is better)\n",
    "    scaler = MinMaxScaler()\n",
    "    yelp_data['location_score'] = 1 - scaler.fit_transform(yelp_data[['distance_km']])\n",
    "\n",
    "    # Normalize business_stars and reviewer_stars to [0, 1]\n",
    "    yelp_data['business_stars_norm'] = yelp_data['business_stars'] / 5\n",
    "    yelp_data['reviewer_stars_norm'] = yelp_data['reviewer_stars'] / 5\n",
    "\n",
    "    # Calculate engagement_score and normalize\n",
    "    yelp_data['engagement_score_raw'] = yelp_data['useful'] + yelp_data['funny'] + yelp_data['cool']\n",
    "    yelp_data['engagement_score'] = scaler.fit_transform(yelp_data[['engagement_score_raw']])\n",
    "\n",
    "    # Calculate preference_rating with non-linear transformations\n",
    "    yelp_data['preference_rating'] = (\n",
    "        weights['location_score'] * apply_non_linear_transformation(yelp_data['location_score']) +\n",
    "        weights['business_stars'] * exponential_boost(yelp_data['business_stars_norm']) +\n",
    "        weights['reviewer_stars'] * angular_score_adjustment(yelp_data['reviewer_stars_norm']) +\n",
    "        weights['engagement_score'] * np.log1p(yelp_data['engagement_score'])\n",
    "    )\n",
    "\n",
    "    # Normalize the preference_rating to 1-10 scale\n",
    "    rating_scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "    yelp_data['preference_rating'] = rating_scaler.fit_transform(\n",
    "        yelp_data[['preference_rating']]\n",
    "    ).round().astype(int)\n",
    "\n",
    "    # Convert business_id to float\n",
    "    yelp_data['business_id'] = yelp_data['business_id'].apply(business_id_to_float)\n",
    "\n",
    "    return yelp_data\n",
    "\n",
    "# Data preparation and display\n",
    "def prepare_final_ranking(yelp_data):\n",
    "    # Remove duplicates, keeping highest-rated business for each name\n",
    "    unique_data = yelp_data.sort_values(by='preference_rating', ascending=False).drop_duplicates(subset='name', keep='first')\n",
    "\n",
    "    # Create ranked dataframe\n",
    "    ranked_yelp = unique_data[['name', 'preference_rating']].sort_values(by='preference_rating', ascending=False)\n",
    "\n",
    "    return ranked_yelp\n",
    "\n",
    "# Process the data\n",
    "yelp_data = generate_preference_rating(yelp_data)\n",
    "\n",
    "# Prepare and display ranking\n",
    "ranked_results = prepare_final_ranking(yelp_data)\n",
    "print(ranked_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52bh8b_gc5q4"
   },
   "source": [
    "Using every feature as input to the training process is not a good idea. Some features are not related to business rating, so let's exclude them from our feature set.\n",
    "1. Extract the following for the input data `X`: `business_stars`,`review_count`,`checkins`,`reviewer_stars`,`useful`,`funny`, and `cool`.\n",
    "2. Store the label variable in `Y`.\n",
    "\n",
    "Let the criteria for a good resturant be a rating greater than **eight**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ibt9CZxSJcCn"
   },
   "outputs": [],
   "source": [
    "# Initialize X and Y\n",
    "# ANSWER STARTING HERE\n",
    "\n",
    "X =\n",
    "Y =\n",
    "\n",
    "# ANSWER ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgOJ0FuQc8jU"
   },
   "source": [
    "3. Use [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from the Scikit-learn module to split the training data using the proposed 80/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1mu13ERc-Ij"
   },
   "outputs": [],
   "source": [
    "seed = 42 # always use this seed number as your random_state in the whole notebook\n",
    "test_size=0.2\n",
    "\n",
    "# Split your training data and use the seed to get consistent results.\n",
    "# ANSWER STARTING HERE\n",
    "\n",
    "X_train, X_test, y_train, y_test =\n",
    "\n",
    "# ANSWER ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv_uQqT0c-o1"
   },
   "source": [
    "4. **Normalize** both the training and test sets so that the model is trained on features with a consistent scale. **You can use any scaler from the Scikit-learn module to do this.** If we don't normalize the data, the model may struggle to converge during training because some features could dominate the loss function due to their larger scale. By normalizing the data, we ensure that each feature contributes equally to the learning process, improving the model's ability to find an optimal solution. (eg. [StandardScaler documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eta7ynDgc_aC"
   },
   "outputs": [],
   "source": [
    "# ANSWER STARTING HERE\n",
    "\n",
    "X_train_scaled =\n",
    "X_test_scaled =\n",
    "\n",
    "# ANSWER ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq1F3ALek_Bc"
   },
   "source": [
    "### **Part C (15 Pts Total)**\n",
    "\n",
    "#### This is going to be the main source of evaluation for the return offer. Be thorough with your model search. You will train a **Logistic Regression (max_iter=1000)** Model, and 3 different classifiers: **KNN**, **DecisionTree**, and **Random Forest**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EG46OGS_qRRs"
   },
   "source": [
    "##### ***3.1 Load Models (4 pts)***\n",
    "\n",
    "---\n",
    "\n",
    "1. Create a python dictionary named `models` that maps the names of the models to their constructors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UszMa88qp2d"
   },
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "# ANSWER STARTING HERE\n",
    "models = {\n",
    "\n",
    "         }\n",
    "\n",
    "# ANSWER ENDS HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNWCxAfHrZQE"
   },
   "source": [
    "##### ***3.2 K-fold Cross-Validation (8 pts)***\n",
    "Using the standardized feature sets, do the following:\n",
    "1. Given five folds and the object with the specified number of splits, perform **k-folds cross-validation** for **each** model using StratifiedKFold.\n",
    "2. Print the **name** of the model and **cross validation accuracy** (mean and standard deviation) for each.\n",
    "\n",
    "[Cross Validation Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8jY2m3erviB"
   },
   "outputs": [],
   "source": [
    "# Perform k-fold Cross-Validation for each model\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    np.random.seed(42)\n",
    "    # ANSWER STARTING HERE\n",
    "\n",
    "    # ANSWER ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeDhkaYLr6gx"
   },
   "source": [
    "##### ***3.3 Question: What is the most accurate ML model from above choice? Why? (3 pts)***\n",
    "ANSWER STARTING HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPJC8UWas1gV"
   },
   "source": [
    "### **Part D (12 Pts Total)**\n",
    "#### We wants you to deploy of all the models to production. Evaluate all of the models on the heldout test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ahUKgJztnjK"
   },
   "source": [
    "##### ***4.1 Model Evaluation (6 pts)***\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "For each of the models do the following in order to evaluate each of them properly:\n",
    "- **Train** each of the models\n",
    "- Print each of the **accuracies** (3 decimal precision) of the models by their **names** in this format: *Accuracy of (model name): (accuracy of the model)*.\n",
    "- Print a **classification report** with the accuracy, precision, recall and f1-score of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgjiE2rwt3TR"
   },
   "outputs": [],
   "source": [
    "# Train each model using the training data\n",
    "for model_name, model in models.items():\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    #ANSWER STARTING HERE\n",
    "\n",
    "\n",
    "    # ANSWER ENDS HERE\n",
    "\n",
    "# Evaluate the performance of each model\n",
    "for model_name, model in models.items():\n",
    "    np.random.seed(42)\n",
    "\n",
    "    #ANSWER STARTING HERE\n",
    "    y_pred =\n",
    "    accuracy =\n",
    "\n",
    "    print(f\"Accuracy of {model_name}: {accuracy:.3f}\") # Your accuracy table header here\n",
    "    print(classification_report(y_test, y_pred)) # Implement your classification report here\n",
    "    # ANSWER ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6D6myn6uHqs"
   },
   "source": [
    "##### ***4.2 Model Interpretation (6 pts)***\n",
    "\n",
    "---\n",
    "\n",
    "- **Interpret** the results of the classification models (list the models which got the best scores for Accuracy, Precision, Recall and F1 score).\n",
    "- **Recommend the best model according to your analysis.** Hopefully your best model has over a 90% accuracy as this will guarantee your return offer.\n",
    "\n",
    "**Question: Which model according to your analysis was the best at predicting the Resturant Preference Rating? Why? DO NOT just say that the accuracy is better than the rest of the models. That is obvious.**\n",
    "\n",
    "ANSWER STARTING HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewX3PJ0owW1q"
   },
   "source": [
    "### **Part E (5 Pts Total)**\n",
    "\n",
    "#### Seeing all the effort and hard work, it looks as if most people have done well with this project🎉! With all the input features you have used, there is still no concrete way of figuring out whether a feature is more important than another. We only really used intuition for that. Thus, we shall do some feature engineering to determine this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeE_vl7CxkQ7"
   },
   "source": [
    "##### ***5.1 What are your top 4 features used to predict Our resturant preference rating (5 pts)?***\n",
    "- Use Random Forest to create a dataframe for **feature importance** named **feature_importance_df**.\n",
    "- **Extract**, **Sort**, and **Print** the **top 4** features from that dataframe.\n",
    "\n",
    "Here are some references to help you:\n",
    "\n",
    "[Feature Importance 1](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_)\n",
    "\n",
    "[Feature Importance 2](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1qoZYbGuNSW"
   },
   "outputs": [],
   "source": [
    "# Extract the feature importances from the most accurate model\n",
    "# ANSWER STARTING HERE\n",
    "importances =\n",
    "\n",
    "# Creating a DataFrame to display feature importances\n",
    "feature_importance_df =\n",
    "\n",
    "# Displaying feature importances\n",
    "\n",
    "\n",
    "# ANSWER ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DijDdJMkJNiR"
   },
   "source": [
    "### Part F (14 Points Total)\n",
    "\n",
    "#Now you'll learn about clustering techniques!\n",
    "\n",
    "You'll perform data preprocessing, apply dimensionality reduction technique, and use K-means clustering to analyze the data. Finally, you'll visualize the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QS9MeP_swG0"
   },
   "source": [
    "##### ***6.1.1 Elbow Method (2.5 pts)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaKGz-8wwsCr"
   },
   "source": [
    "In K-Means clustering, we start by randomly initializing k clusters and iteratively adjusting these clusters until they stabilize at an equilibrium point. However, before we can do this, we need to decide how many clusters (k) we should use.\n",
    "\n",
    "In this part, you will determine the optimal number of clusters for a given dataset using the **Elbow Method**. The goal is to identify the point where adding more clusters results in minimal improvement in clustering quality, known as the \"elbow point.\"\n",
    "\n",
    "The *distortion* for a given number of clusters $k$ can be represented as:\n",
    "\n",
    "\n",
    "\n",
    "$$D(k) = \\frac{1}{N} \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2$$\n",
    "\n",
    "Where:\n",
    "- $D(k)$ is the distortion for $k$ clusters\n",
    "- $k$ is the number of clusters\n",
    "- $C_i$ represents the $i$-th cluster\n",
    "- $x$ is each data point in the cluster\n",
    "- $\\mu_i$ is the centroid (mean) of the $i$-th cluster\n",
    "- $\\|\\cdot\\|$ denotes the Euclidean distance\n",
    "\n",
    "\n",
    "In the Elbow Method, the Inertia is a measure of the sum of squared distances between data points and their respective cluster centers. It's often used to evaluate the compactness of clusters and is defined as:\n",
    "\n",
    "$$I(k) = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2$$\n",
    "\n",
    "\n",
    "Task A: Implement your own distortion function to determine the different numbers of clusters, ranging from 1 to 10. Plot the elbow plot and guess the elbow point $k$. Use X_Train that we calculated in the previous parts for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVm1RGO4wsb4"
   },
   "outputs": [],
   "source": [
    "distortions = []\n",
    "inertias = []\n",
    "K = range(1, 10)\n",
    "\n",
    "for k in K:\n",
    "    # ANSWER STARTING HERE\n",
    "\n",
    "\n",
    "\n",
    "# ANSWER ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsYt3IT6xk0v"
   },
   "source": [
    "To determine the optimal number of clusters, we have to select the value of $k$ at the “elbow”, i.e. the point after which the distortion starts decreasing in a linear fashion. Thus for the given data, we conclude that the optimal number of clusters for the data is 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fk1VLV8ZuNTd"
   },
   "outputs": [],
   "source": [
    "k_cluster = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtHyk_fTs-sH"
   },
   "source": [
    "##### ***6.1.2 KElbowVisualizer (2.5 pts)***\n",
    "Task B: Use the `KElbowVisualizer` from the `yellowbrick` library to verify the conclusion above. See [Elbow Method\n",
    "](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDXLXpIoyJqR"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# ANSWER STARTING HERE\n",
    "# Load the dataset\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "\n",
    "\n",
    "# ANSWER ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWSe5oOSuk_m"
   },
   "source": [
    "##### ***6.1.3 Question: According to the plots above, why do you think we chose K=3? (1 pts)***\n",
    "\n",
    "ANSWER STARTING HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQjLyt-fUirG"
   },
   "source": [
    "##### ***6.2.1 Extract top features (1 pts)***\n",
    "Extract the top 4 features you defined in Part E for show its clustering for the following steps. Name your feature dataframe `X`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uv7p-JKfPRmz"
   },
   "outputs": [],
   "source": [
    "# TODO: Select most relevant features for clustering (exclude 'rating')\n",
    "data = yelp_data\n",
    "\n",
    "# ANSWER STARTING HERE\n",
    "features = [ ]\n",
    "X =\n",
    "# ANSWER ENDS HERE\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWHD88oXQgfm"
   },
   "source": [
    "We wish to standardize the feature sets in order to make sure that the PCA is not biased by differences in scales within the data. Standardization aids with consistency in the data.\n",
    "\n",
    "##### ***6.2.2 Normalize the data using StandardScaler. (1 pts)***\n",
    "\n",
    "- **Standardize** the features and save it as `X_scaled`.\n",
    "\n",
    "*Hint:* [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZPuZnZvQg2w"
   },
   "outputs": [],
   "source": [
    "# TODO: Normalize the data using StandardScaler\n",
    "# ANSWER STARTING HERE\n",
    "X_scaled =\n",
    "# ANSWER ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpCufLDfPMaU"
   },
   "source": [
    "There are varying reasons for using a dimensionality reduction step such as PCA prior to data segmentation. By reducing the number of features, we are improving the performance of our algorithm. In addition, by decreasing the number of features the noise is also reduced.\n",
    "\n",
    "#### PCA\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that finds directions (principal components) maximizing variance in a dataset. It's an orthogonal linear transformation mapping data to a new coordinate system [1].\n",
    "\n",
    "PCA steps for a dataset $X$ with $n$ samples and $p$ features:\n",
    "\n",
    "1. Center the data.\n",
    "2. Compute covariance matrix $\\Sigma = \\frac{1}{n-1}X^TX$.\n",
    "3. Calculate eigenvectors and eigenvalues of $\\Sigma$.\n",
    "4. Sort eigenvectors by descending eigenvalues.\n",
    "5. Select top $k$ eigenvectors as new basis.\n",
    "\n",
    "Project original data onto this new basis for transformed data.\n",
    "\n",
    "PCA is simple and effective for linear relationships but may not suit complex, non-linear structures [2].\n",
    "\n",
    "For more in-depth reading on PCA:\n",
    "\n",
    "[1] Scikit-learn PCA documentation: [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "[2] Jolliffe, I. T. (2002). Principal Component Analysis, Second Edition. Springer Series in Statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jM7P-32wUoJN"
   },
   "source": [
    "##### ***6.3.1 Use a PCA function to return the transformed data. (3 pts)***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J78YGqW7JOZN"
   },
   "outputs": [],
   "source": [
    "# TODO: Apply PCA and return the transformed data\n",
    "\n",
    "def apply_pca(X_scaled, n_components=2):\n",
    "    # ANSWER STARTING HERE\n",
    "\n",
    "\n",
    "    # ANSWER ENDS HERE\n",
    "    return pca, X_pca\n",
    "    # Apply PCA\n",
    "\n",
    "pca, X_pca = apply_pca(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1n9iyS0Tfd6"
   },
   "source": [
    "#### K-Means Clustering\n",
    "\n",
    "K-Means is a popular unsupervised learning algorithm used for clustering, where the goal is to partition data into K clusters based on feature similarity. The algorithm works by iteratively assigning data points to the nearest centroid (cluster center) and updating centroids to minimize the variance within each cluster. It starts by randomly initializing centroids and repeats the process until the centroids stabilize (convergence). K-Means is efficient for large datasets but sensitive to the choice of K and initial centroids. In your homework, you'll implement K-Means to explore how well it groups your data based on similarity!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_2zyJBOVQPN"
   },
   "source": [
    "##### ***6.4 Apply K-means clustering and return the cluster labels (3 pts)***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1i_57R6bTf5I"
   },
   "outputs": [],
   "source": [
    "# TODO: Apply K-means clustering and return the cluster labels\n",
    "def apply_kmeans(X, n_clusters, random_state=seed):\n",
    "    # ANSWER STARTING HERE\n",
    "\n",
    "\n",
    "    # ANSWER ENDS HERE\n",
    "    return labels\n",
    "\n",
    "# Apply K-means to both PCA results\n",
    "pca_labels = apply_kmeans(X_pca, n_clusters = k_cluster)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db_x7pbsVZlF"
   },
   "source": [
    "Visualize your clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlXe5QbOVZ_k"
   },
   "outputs": [],
   "source": [
    "## Step 6: Visualization\n",
    "\n",
    "def plot_clusters(X, labels, rating, title, xlabel, ylabel):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    kmeans = KMeans(n_clusters=k_cluster, random_state=seed).fit(X)\n",
    "    data['Cluster'] = pd.Categorical(kmeans.labels_)\n",
    "    sns.scatterplot(x=X[:, 0],y=X[:, 1], hue=\"Cluster\", data=data,s=45,legend=\"full\")\n",
    "    plt.legend(labels=[features[0], features[1], features[2], features[3]])\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "print(X_pca)\n",
    "print(X_pca.shape)\n",
    "print(data['preference_rating'].shape)\n",
    "\n",
    "# Visualize PCA results\n",
    "plot_clusters(X_pca, pca_labels, data['preference_rating'],\n",
    "              'PCA: Clusters and Rating', 'PC1', 'PC2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsC_qOYeNXka"
   },
   "source": [
    "# Your analysis here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPkoHTxM5mBv"
   },
   "source": [
    "**You have been promoted!!**\n",
    "\n",
    "**You are now one of the consulting company's elite employees! 🎊🎉🥂🥳**\n",
    "\n",
    "![](https://media.giphy.com/media/JIX9t2j0ZTN9S/giphy.gif?cid=790b76111hlwamin05rcv1ujslgjh5o49haj07h4wpuvwkgz&ep=v1_gifs_search&rid=giphy.gif&ct=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaD19-nrcWVh"
   },
   "source": [
    "### Extra Credit ✨ (7 points total)\n",
    "The matplotlib and seaborn libraries are great tools for visualizing data.\n",
    "\n",
    "They are used to create legible and complex graphs that are essential in data interpretation and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i-1gG-55mBx"
   },
   "source": [
    "##### ***7.1 Visualize the distribution of each feature in the original dataset for different rating classes (3 pts)***\n",
    "- **Plot** out histograms for each feature. You can use a for loop.\n",
    "- Label the **title** of each histogram \"Distribution of (Feature Name) by Rating\"\n",
    "- Label the **axes** appropriately and display each histogram.\n",
    "- Display the **y-axis** using a **logarithmic** scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GSTkDYW5mBy"
   },
   "outputs": [],
   "source": [
    "# Dropping text or non-numeric columns\n",
    "histogram_pd = yelp_data.drop(columns=[\n",
    "    'business_id', 'name', 'neighborhood', 'latitude', 'longitude','address', 'city', 'state',\n",
    "    'postal_code', 'categories', 'review_id', 'user_id', 'text'\n",
    "])\n",
    "\n",
    "histogram_pd = histogram_pd.iloc[:, list(range(8)) + [-2]]\n",
    "histogram_pd.head()\n",
    "\n",
    "# ANSWER STARTING HERE\n",
    "# Histograms for each feature with rating hue\n",
    "for feature in histogram_pd.columns[:-1]:  # Exclude 'rating' column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create the histogram\n",
    "\n",
    "    # Create a custom legend\n",
    "\n",
    "# ANSWER ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHUCbckL5mBz"
   },
   "source": [
    "##### ***7.2 Explore correlations between features and Rating ratings (2 pts)***\n",
    "- Create a **correlation matrix** based on the correlations between features and rating.\n",
    "- **Display** the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTh5Yf0p5mB0"
   },
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "# ANSWER STARTING HERE\n",
    "\n",
    "# ANSWER ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfsOf2r5aMPN"
   },
   "source": [
    "##### **7.3 *Question*: What have you learned from this correlation map? Why? (2 pts)**\n",
    "ANSWER RIGHT HERE!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XkmRz1vaRfw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "uWIXqii1puOD",
    "oq1F3ALek_Bc",
    "TPJC8UWas1gV",
    "2QS9MeP_swG0",
    "QtHyk_fTs-sH",
    "rQjLyt-fUirG",
    "MWHD88oXQgfm",
    "jM7P-32wUoJN"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
